# Arxiv

1.对数据集做处理，又删除了部分不合适的样本。（原来是尽可能地保留，这次是能删就删，尽量精简）。

这次选了10W条来跑实验，效果依然不好。结果更新到了原实验文档上。

后续再考虑，陆续地加一些段落回来，看看效果。

2.用精简数据集实验了纯无监督生成摘要，也就是我自己想到的那个算法。这个方法被Pass，算法设计的还是过于简单。

3.根据rouge分，构建了一个应用于抽取阶段的“伪数据集”。（下文中详细介绍）



# 司法摘要

总体做法还是两段式：抽取---生成



## 抽取阶段

### 一、选句子策略

在文本摘要任务中，所有数据集中给出的格式都为`原文---最终摘要`这一形式。所以对于抽取摘要这一过程，并没有给出明确的标签。也就是说明确告诉我们，哪一句该抽取，哪一句不该要。诸如下面的格式。

```xml
原文：{s1 s2 s3 s4 s5}
抽取：{1  0  0  0  1 }
```

所以会在原数据集上进行处理，通过**某种规则**，建立一个`原文--抽取摘要`的"伪数据集"。

这个**规则**可取很多种方式，比如：

* 最简单的做法，根据贪心算法的思想。对原数据集中参考摘要中的每一句，我们都在原文中进行匹配，找到**相似度**得分最高的句子，我们给这个句子打上`标签1`。最后会得到一个和原文句子数量相同长度的标签集{1，0，1，......}，作为我们的伪数据集。

  ```java
  for (参考摘要句x ： 参考摘要句集) {
      for (原文句y ： 原文句集) {
          s = max(s, fun(x,y));
      }
  }
  //本质两个for循环，O(n^2)复杂度
  ```

  这是采用最多的一种方式，因为足够简单，计算也快。单独来看参考摘要中的每个句子，和与之对应的在原文中得分最高的句子都被选了出来，也就是一定能保证对于参考摘要中的每个句子，我都能选出原文中对应的

  **缺点：**

  1. 并不能代表整体的语义就是最高。也就是说，我选出的句子集整体和参考摘要整体，做一个相似度计算，不一定是最高的。
  2. 句子数量固定，只能选出和参考摘要一样的句子数。**如果我参考摘要中的一句，可以由原文中的两句来组成呢？（这个问题可以研究一下）**

* TopK方法。这个方法的搜索空间比上述贪心大一点，以Top3为例。

  1. 从摘要的第一句开始，遍历原文，选出与其相似度最高的3个句子，作为候选。C1={s1,s2,s3}

  2. 对于摘要的第二句，遍历原文，每句都分别和已选出三个的集合中的句子组合。计算和参考摘要前两句的相似度，再选出3个最高。

  3. 循环下去，直到选出N句。主要是为了减少搜索空间。

     以上是当选第i句时，以参考摘要的前i句为对比。

     选第三句时，要保证，这个句子加入已选集合中后，已选集合和参考摘要前三句的相似度是最高的。

     还可以这样做，每次选出都和参考摘要的全文对比。

* 列出所有组合的可能，一定能取到全局最优。但是长文本句子数太多，导致搜索空间爆炸，不现实。

......



**总之，以上所有做的事情，都是为了构建出一个高质量的，为抽取阶段做准备的“伪数据集”。**

选句子的算法不同都会对质量产生影响，而且还要根据数据集的特点做选择。

如果你的参考摘要，每一句都能在原文中找到明显的对应，按照单句选好一点。如果你的数据集抽象度高，经常需要文中几句组合，压缩才能形成相应的某句摘要，那么就要从全局来开率了。这里也要做数据分析工作。



### 二、相似度计算

另外，在计算相似度方面也可以做文章。我了解到的都是直接以Rouge分来算，因为Rouge分就是最后的评判标准，直接以两句的Rouge分高低来判断选不选也是合理的。在中文中，又可以以字来算Rouge，也可以以词、实体来算，**这里也可以加对照实验。**

除了以Rouge分，也可以用一些训练好的模型来算出两句相似度，以此为标准，或者以上组合加权重。（还没人这样做过）。

### 三、抽取模型的选择

文章是把抽取阶段看作一个**序列标注问题**，选择了Bert+平均池化来表示句向量，然后抽取时，用了**DGCNN**这个改动版的CNN。

看了看这个DGCNN，感觉作者用它很大一部分的原因是：这个DGCNN是他造出来的，他在其他比赛上也用过，所以他要沿用。

（DGCNN刚开始出来是打阅读理解任务比赛时用的，炜博可以了解一下）

这里其实也可以继续做下去，这部分可以用RNN、LSTM，再改注意力呀。好多种可选的模型，还有好多可改动优化的地方。因为是个比赛，估计作者也没办法去一一验证。

文章采用的是以句子为单位进行抽取，其实以词和短语来抽也可以尝试，用图神经网络？

## 生成阶段

生成阶段最大的问题就是长度问题了。Longformer、BigBird、天马这种都没有针对中文的预训练版本。自己去预训练一个也不现实。

所以要开始魔改了，文中选的是华为哪吒为基础，在此之上再改动。（相对编码部分）

这一部分也是有很多可以做的，我也可以改注意力啊，复制的方式等等。而且也可以再读文章，找合适的中文预训练模型。



**个人想法还有：**

生成阶段还是和前一阶段选择的策略有关系的。

我觉得单句最优策略，适合后面再接一个单句改写/压缩模型。

而全局最优策略，适合接一个全文的生成式模型。

* 这个数据集，格式比较统一规范，而且法律文本也不像新闻等需要高度抽象。参考摘要中的句子，在原文中都能很好地找到对应。（这个还需进一步数据统计分析）。这样的话，可以尝试一下单句改写/压缩这种方式。缺点是，照顾不到摘要中一句对应文中两句这种情况。但是这种情况出现得多不多呢，比例是多少？（也要数据分析）
* 抽取阶段强化学习的方式也可以用一下。
* 向用一个全文的生成式模型来对比结果，就是输入长文本，直接纯生成式来做。英文中有Bigbird和天马可用，中文情况还没人这么做，但是我们也预训练不了。



# 小结

两个数据集都会继续跟进。

Arxiv这个数据集，

难度主要在：

* 数据量太大，原数据集的长度（超过6000）和样本数（20W），用精简后（2000长度）的跑一次也要花上十几个小时。用的还是原文中的方法，只是LSTM。如果再用上参数量更大的模型、Longformer、Bigbird这种，是肯定跑不了的。

  如果就只是用其中几万条数据来跑可以接受，但是效果肯定比不了他们。感觉这个数据集就适合于堆模型参数，拼计算量这种。因为BigBird、天马都是这样做的。

  自己精简数据集，这个数据集形式太杂了，完全不知道精简后的数据集质量，不知道该删哪该保留哪。做错误分析也难做，还是数据量太大的问题，二十万数据不可能一条一条来对照着看。而且这个是英文的，里面夹杂着公式、图片、表格等转化信息，不像中文，看起来一目了然。



司法摘要这个数据集

难度主要在：

* 中文，天然就比英文更难。

* 虽然我感觉这个数据集的质量挺好的，也是少有的中文长文本摘要。但是它毕竟不是顶会文章中使用的数据集，如果做出来结果认可度怎么样？还有一点就是，给的基线模型只是Lead-3，对比的不全面，想要对比更多的基线模型，要自己复现。